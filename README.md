# ğŸ“˜ PDF RAG System using Ollama & Vector Search

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that allows you to ask questions from a PDF document using **semantic search** and a **local LLM (Ollama)**.

The system retrieves the most relevant chunks from the PDF using vector similarity and generates answers strictly grounded in the document content.

---

## ğŸš€ Features

- ğŸ“„ Load and process PDF files
- âœ‚ï¸ Chunk documents for better retrieval
- ğŸ”¢ Generate embeddings using Sentence Transformers
- ğŸ§  Semantic search using a vector index
- ğŸ¤– Local LLM inference using **Ollama (Gemma / LLaMA)**
- âŒ No OpenAI / cloud dependency
- âœ… Answers grounded only in PDF context

---

## ğŸ§  Architecture
User Question
â†“
Sentence Embeddings
â†“
Vector Database (Semantic Search)
â†“
Top-k Relevant PDF Chunks
â†“
Ollama LLM
â†“
Final Answer (from PDF only)


---

## ğŸ“¦ Tech Stack

- **Python**
- **LangChain**
- **Sentence-Transformers**
- **Ollama**
- **Vector Database** (Endee / FAISS / similar)

---


---

## âš™ï¸ Setup Instructions

### Install dependencies

```bash
pip install -r requirements.txt
```

### Start Ollama

```bash
ollama pull gemma3:4b
ollama serve
```

